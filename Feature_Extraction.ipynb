{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2e9b41-2c43-4be5-9b00-a10058b8cf2b",
   "metadata": {},
   "source": [
    "# 1 one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb3e321-b2f3-48d7-9170-bd58e669428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "588a2387-116c-4ccf-8e1c-2a55a7011617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text\n",
      "0   1                            I love machine learning\n",
      "1   2                        Machine learning is amazing\n",
      "2   3  Natural language processing is part of machine...\n",
      "3   4                         I love learning new things\n"
     ]
    }
   ],
   "source": [
    "# Sample text data\n",
    "data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'text': [\n",
    "        \"I love machine learning\",\n",
    "        \"Machine learning is amazing\",\n",
    "        \"Natural language processing is part of machine learning\",\n",
    "        \"I love learning new things\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "365889ea-5e9e-4a81-b257-c61da58b029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amazing  is  language  learning  love  machine  natural  new  of  part  \\\n",
      "0        0   0         0         1     1        1        0    0   0     0   \n",
      "1        1   1         0         1     0        1        0    0   0     0   \n",
      "2        0   1         1         1     0        1        1    0   1     1   \n",
      "3        0   0         0         1     1        0        0    1   0     0   \n",
      "\n",
      "   processing  things  \n",
      "0           0       0  \n",
      "1           0       0  \n",
      "2           1       0  \n",
      "3           0       1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# One-Hot Encoding using CountVectorizer (binary=True)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_onehot = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "onehot_df = pd.DataFrame(X_onehot.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(onehot_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d3b53-b51e-4790-8fb7-e8bd014604bc",
   "metadata": {},
   "source": [
    "# 2 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54002843-0126-41c5-88c0-9555e078181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amazing  is  language  learning  love  machine  natural  new  of  part  \\\n",
      "0        0   0         0         1     1        1        0    0   0     0   \n",
      "1        1   1         0         1     0        1        0    0   0     0   \n",
      "2        0   1         1         1     0        1        1    0   1     1   \n",
      "3        0   0         0         1     1        0        0    1   0     0   \n",
      "\n",
      "   processing  things  \n",
      "0           0       0  \n",
      "1           0       0  \n",
      "2           1       0  \n",
      "3           0       1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2864494c-71e4-4357-a080-b8adcbe54bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([\"I love machine learning deep learning\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db83672-33df-4663-be7c-7baf649fb72e",
   "metadata": {},
   "source": [
    "Note: Words like \"deep\" won't appear unless they're in the vocabulary (fitted corpus). Since \"deep\" isn't in the original training data, it will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd2fa13c-a4b5-4a0e-9a3a-7a13d9b692cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer1 = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit on your original DataFrame text\n",
    "vectorizer1.fit(df['text'])\n",
    "\n",
    "# Transform a new sentence\n",
    "vectorizer1.transform([\"I love machine learning deep learning\"]).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e2f7b-4bc7-423c-bdd9-e23be7e9eb5f",
   "metadata": {},
   "source": [
    "# n grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f572aae-a40b-4615-ba2f-e6f1e085103a",
   "metadata": {},
   "source": [
    "### bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11da89a7-2b49-4a26-9151-9a094f5609b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love machine': 6, 'machine learning': 7, 'learning is': 3, 'is amazing': 0, 'natural language': 8, 'language processing': 2, 'processing is': 12, 'is part': 1, 'part of': 11, 'of machine': 10, 'love learning': 5, 'learning new': 4, 'new things': 9}\n",
      "[[0 0 0 0 0 0 1 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Use bigrams (2-grams) and binary=True\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit on original text data\n",
    "vectorizer.fit(df['text'])\n",
    "\n",
    "# Transform a new sentence\n",
    "X_test = vectorizer.transform([\"I love machine learning deep learning\"]).toarray()\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "# Output: {'i love': 0, 'love machine': 1, 'machine learning': 2, ...} in dict form\n",
    "\n",
    "# Show the feature names for clarity\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c31d4f95-e2fa-4c4d-a3fd-12d0533ce318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 9, 'machine': 12, 'learning': 6, 'love machine': 11, 'machine learning': 13, 'is': 1, 'amazing': 0, 'learning is': 7, 'is amazing': 2, 'natural': 14, 'language': 4, 'processing': 22, 'part': 20, 'of': 18, 'natural language': 15, 'language processing': 5, 'processing is': 23, 'is part': 3, 'part of': 21, 'of machine': 19, 'new': 16, 'things': 24, 'love learning': 10, 'learning new': 8, 'new things': 17}\n",
      "[[0 0 0 0 0 0 2 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Use bigrams (2-grams) and uni-gram(single)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit on original text data\n",
    "vectorizer.fit(df['text'])\n",
    "\n",
    "# Transform a new sentence\n",
    "X_test = vectorizer.transform([\"I love machine learning deep learning\"]).toarray()\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9027766b-17bc-497f-ada5-8b3d5693a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love machine learning': 5, 'machine learning is': 6, 'learning is amazing': 2, 'natural language processing': 7, 'language processing is': 1, 'processing is part': 10, 'is part of': 0, 'part of machine': 9, 'of machine learning': 8, 'love learning new': 4, 'learning new things': 3}\n",
      "[[0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Use tri-grams (2-grams)\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "# Fit on original text data\n",
    "vectorizer.fit(df['text'])\n",
    "\n",
    "# Transform a new sentence\n",
    "X_test = vectorizer.transform([\"I love machine learning deep learning\"]).toarray()\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "596df6f8-906f-4e6c-9ed9-76bf8c484851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 13, 'machine': 18, 'learning': 8, 'love machine': 16, 'machine learning': 19, 'love machine learning': 17, 'is': 1, 'amazing': 0, 'learning is': 9, 'is amazing': 2, 'machine learning is': 20, 'learning is amazing': 10, 'natural': 21, 'language': 5, 'processing': 32, 'part': 29, 'of': 26, 'natural language': 22, 'language processing': 6, 'processing is': 33, 'is part': 3, 'part of': 30, 'of machine': 27, 'natural language processing': 23, 'language processing is': 7, 'processing is part': 34, 'is part of': 4, 'part of machine': 31, 'of machine learning': 28, 'new': 24, 'things': 35, 'love learning': 14, 'learning new': 11, 'new things': 25, 'love learning new': 15, 'learning new things': 12}\n",
      "[[0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Use tri-grams (3-grams) and uni-gram(single)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Fit on original text data\n",
    "vectorizer.fit(df['text'])\n",
    "\n",
    "# Transform a new sentence\n",
    "X_test = vectorizer.transform([\"I love machine learning deep learning\"]).toarray()\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a36fd-15aa-4210-bbef-958875ce9027",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fa2df00-8fbd-4a44-99a5-084e043fb671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.51082562 1.91629073 1.         1.51082562 1.22314355\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073]\n",
      "['amazing' 'is' 'language' 'learning' 'love' 'machine' 'natural' 'new'\n",
      " 'of' 'part' 'processing' 'things']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_tfidf = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "print(vectorizer.idf_)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14b2c03c-f0d8-40aa-98b5-077d5bf192b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    amazing        is  language  learning      love   machine   natural  \\\n",
      "0  0.000000  0.000000  0.000000  0.457453  0.691131  0.559530  0.000000   \n",
      "1  0.659191  0.519714  0.000000  0.343993  0.000000  0.420753  0.000000   \n",
      "2  0.000000  0.314078  0.398368  0.207885  0.000000  0.254273  0.398368   \n",
      "3  0.000000  0.000000  0.000000  0.306758  0.463458  0.000000  0.000000   \n",
      "\n",
      "        new        of      part  processing    things  \n",
      "0  0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "2  0.000000  0.398368  0.398368    0.398368  0.000000  \n",
      "3  0.587838  0.000000  0.000000    0.000000  0.587838  \n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame for easier viewing\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815ec35-f2df-44e4-b683-6ed7ff2c8f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
